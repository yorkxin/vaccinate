# This task runs scrapers and uploads data to a Dropbox location
name: Refresh

on:
  # schedule:
  #   - cron: '0,30 * * * *'

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  scraper:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2
        with:
          path: main

      - uses: actions/checkout@v2
        with:
          ref: gh-pages
          path: gh-pages

      - uses: actions/setup-python@v1

      - name: Install Dependencies
        working-directory: ./main
        run: |
          python -m pip install pipenv

      - uses: actions/cache@v1
        with:
          path: ~/.local/share/virtualenvs
          key: ${{ runner.os }}-pipenv-${{ hashFiles('**/Pipfile.lock') }}

      - run: python -m pipenv install --dev
        working-directory: ./main

      - name: Run Scraper
        working-directory: ./main
        run: |
          pipenv run python backend/local_scraper.py > availability.json

      - name: Commit into gh-pages branch
        working-directory: ./gh-pages
        run: |
          git config --local user.name "scraper by yorkxin"
          git config --local user.email "ducksteven@gmail.com"

          cp ../main/availability.json ./data/availability.json
          git add ./data/availability.json
          git commit -m "Refresh at $(date)"
          git push
