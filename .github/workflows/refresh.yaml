# This task runs scrapers and uploads data to a Dropbox location
name: Refresh

on:
  # schedule:
  #   - cron: '0,30 * * * *'

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  scraper:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2
        with:
          path: main

      - uses: actions/checkout@v2
        with:
          ref: gh-pages
          path: gh-pages

      - uses: actions/setup-python@v1

      - name: Install Dependencies
        working-directory: ./main
        run: |
          python -m pip install pipenv

      - uses: actions/cache@v1
        with:
          path: ~/.local/share/virtualenvs
          key: ${{ runner.os }}-pipenv-${{ hashFiles('**/Pipfile.lock') }}

      - run: python -m pipenv install --dev
        working-directory: ./main

      - name: Run Scraper
        working-directory: ./main
        run: yarn run scrape

      - name: Commit into gh-pages branch
        working-directory: ./gh-pages
        run: |
          git config --local user.name "Builder by ${{ github.actor }}"
          git config --local user.email "noreply@g0v.tw"

          cp -R ../main/public/data/scraped/* ./data/scraped/
          git add ./data/scraped/
          git commit -m "Refresh at $(date)"
          git push
